{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3533e1f9-3f47-41b4-bc64-f8587aa896c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 0: Depedencies and Demo Data\n",
    "\n",
    "We need to install the necessary libraries and some demo data for the demo to run properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc63da2-5d02-4905-855b-73437795b857",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Dependencies and Sample Data"
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08acbbed-28df-46d7-a76e-4d7619480307",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up Vector Search Index (If needed)"
    }
   },
   "outputs": [],
   "source": [
    "%run ./rag_setup/rag_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fec98ad3-2e9d-42a4-ad1a-28df90db50b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Disclaimer \n",
    "\n",
    "In order for Langgraph to run, we need to write the code into a separate file called **agent.py**. We will go through each step of setting up Langgraph in pieces to exaplin what is happening, then combine all of it in one cell to create the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c818b8fb-19d3-420d-af3a-4e72b01616f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#What is Langgraph? \n",
    "\n",
    "Langgraph is a stateful, agentic framework from Langchain that simplifies agent development. It allows developers to create complex agent workflows by organizing language model calls, tool use, and memory into structured, composable components that can be deployed as robust applications. LangGraph's key innovation is enabling dynamic, multi-step interactions between different \"agents\" while maintaining contextual awareness and providing clear visualization of the application's flow.\n",
    "\n",
    "We will break each portion of Langgraph out to demonstrate this structure and explain what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6634ce27-115c-47fb-86d4-e499ee459a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 0: Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfda43c5-1572-4fba-99ee-fb954a0aecc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "from config_import import demo_schema, catalog, chatBotModel, vectorSearchIndexName, embeddings_endpoint, agent_schema #we have some default configurations that we will import. You are welcome to change these. \n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    "    DatabricksEmbeddings\n",
    ")\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "from unitycatalog.ai.core.base import set_uc_function_client\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25869688-f106-484f-a859-3026e544cd7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 1: Define your LLM Endpoint and System Prompt\n",
    "\n",
    "We need to specify what LLM we plan to use for this application. On Databricks, any LLM that you host on Model serving, be it an open source model or external model, can be used. Model Serving handles compatibility between different providers allowing you to use any model you bring to Databricks \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c3113a-6e92-4000-be34-7c20d031d73d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# chatBotModel = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "LLM_ENDPOINT_NAME = chatBotModel\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME) #Langgraph's implementation on what model to use\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = f\"\"\"## Instructions for Testing the Databricks Documentation Assistant chatbot\n",
    "\n",
    "Your inputs are invaluable for the development team. By providing detailed feedback and corrections, you help us fix issues and improve the overall quality of the application. We rely on your expertise to identify any gaps or areas needing enhancement.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d4815d4-86ca-4ecb-b9e0-78025bca93ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 2: Define our Tools\n",
    "\n",
    "### Mosaic AI Tools on Unity Catalog\n",
    "\n",
    "You can create and host functions/tools on Unity Catalog! You get the benefit of Unity Catalog but for your functions! \n",
    "\n",
    "While you can create your own tools using the same code that you built your agent (i.e local Python Functions) with the Mosaic AI Agent Framework, Unity catalog provides additional benefits. Here is a comparison \n",
    "\n",
    "1. **Unity Catalog function**s: Unity Catalog functions are defined and managed within Unity Catalog, offering built-in security and compliance features. Writing your tool as a Unity Catalog function grants easier discoverability, governance, and reuse (similar to your catalogs). Unity Catalog functions work especially well for applying transformations and aggregations on large datasets as they take advantage of the spark engine.\n",
    "\n",
    "2. **Agent code tools**: These tools are defined in the same code that defines the AI agent. This approach is useful when calling REST APIs, using arbitrary code or libraries, or executing low-latency tools. However, this approach lacks the built-in discoverability and governance provided by Unity Catalog functions.\n",
    "\n",
    "Unity Catalog functions have the same limitations seen here: https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html \n",
    "\n",
    "Additionally, the only external framework these functions are compatible with is Langchain \n",
    "\n",
    "So, if you're planning on using complex python code for your tool, you will likely just need to create Agent Code Tools. \n",
    "\n",
    "###Langgraph Implementation\n",
    "For Langgraph, we need to define and list our tools out before hand so that we can pass it into the application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58195ce4-2aca-46d1-bf0c-9646cb236992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###First, let's define our own UDFs or Unity Catalog Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d293a28b-fbb4-4701-a220-57bf9181b398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION identifier(CONCAT(:catalog_name||'.'||:agent_schema||'.','purchase_location'))()\n",
    "    RETURNS Table(name STRING, purchases INTEGER)\n",
    "    COMMENT 'Use this tool to find total purchase information about a particular location. This tool will provide a list of destinations that you will use to help you answer questions. Only use this if the user asks about locations.'\n",
    "    RETURN SELECT dl.name AS Destination, count(tp.destination_id) AS Total_Purchases_Per_Destination\n",
    "             FROM main.dbdemos_fs_travel.travel_purchase tp join main.dbdemos_fs_travel.destination_location dl on tp.destination_id = dl.destination_id\n",
    "             group by dl.name\n",
    "             order by count(tp.destination_id) desc\n",
    "             LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3735cb85-c387-4ad1-9fe2-2d228939a5b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Next, let's add to as one of the tools that Langgraph Agent Can call\n",
    "\n",
    "We need to specific the location of the function using catalog.schema.tool_name, which we set in the configuration file. The * simply means that we want the agent to be able to use all UDFs located in that schema. If you have multiple functions, it will considerr all the functions for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3a5143-c6d8-4b6d-9a00-773fee5614f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## To create and see usage examples of more tools, see https://docs.databricks.com/en/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "tools = []\n",
    "\n",
    "# Add additional tools\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "uc_tool_names = [f\"{catalog}.{agent_schema}.*\"] # you can specify individual tools as list but we are going to select all for this demo\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "# Use Databricks vector search indexes as tools\n",
    "# See https://docs.databricks.com/en/generative-ai/agent-framework/unstructured-retrieval-tools.html for details\n",
    "\n",
    "# Add vector search indexes, we set this up using rag_setup. \n",
    "vector_search_tools = [\n",
    "        VectorSearchRetrieverTool(\n",
    "          index_name=f\"{catalog}.{demo_schema}.{vectorSearchIndexName}\",\n",
    "          tool_name=\"databricks_docs_retriever\",\n",
    "          tool_description=\"Retrieves information about Databricks products from official Databricks documentation.\",\n",
    "          columns=[\"id\", \"url\", \"content\"],\n",
    "          embedding=DatabricksEmbeddings(endpoint=embeddings_endpoint),\n",
    "          text_column=\"content\",\n",
    "        )\n",
    "]\n",
    "tools.extend(vector_search_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ead6d8f7-22e3-4a92-9445-9323d780ed7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 3: Define the Agent's Logic\n",
    "\n",
    "Now we have to set up the code that dictates the logic the Agent will go through. Langgraph has a concept called Nodes and Edges that help dictate and enforce what step an Agent should take next which could be a new Agent or a new tool. We compile this Langgraph workflow at the end to represent the state of the graph. Between each node, the ChatAgentState is passed so that the Agent remembers what has happened previously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f182a613-388f-4df5-a56d-eb79523ebc3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6174829-23a1-4f1b-b8fa-3821be76bd7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 4: Create a ChatAgent Model\n",
    "\n",
    "ChatAgent is a new MLflow Interface that enforces a chat schema to author conversational agents. It is an important step in reviewing our Agent's performance, confirm tools calls and support multi-agent scenarios. We will see this in action with Databrick's Review App. \n",
    "\n",
    "Check out MLflow's documentaton here: https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatAgent\n",
    "\n",
    "We complete the setup by telling MLflow where the Agent configuration is located and enabling mlflow.langchain.autolog which include Langgraph. This enables MLflow traces, experiments and other logging capabilities critical in managing our Agent in production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1891f428-65bd-48cf-ba8f-6309b4e5463c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d22eb3-87f9-42bc-87c0-f0c79f9903bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Completed Code\n",
    "\n",
    "Now we run all our code to create an agent.py file that we will use to run the Langgraph agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca82b61-6e84-4785-b286-2ed2cd1ca190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "from config_import import demo_schema, catalog, chatBotModel, vectorSearchIndexName, embeddings_endpoint, agent_schema\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    "    DatabricksEmbeddings\n",
    ")\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "from unitycatalog.ai.core.base import set_uc_function_client\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "# TODO: Replace with your model serving endpoint\n",
    "# LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "LLM_ENDPOINT_NAME = chatBotModel\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = f\"\"\"## Instructions for Testing the Databricks Documentation Assistant chatbot\n",
    "\n",
    "Your inputs are invaluable for the development team. By providing detailed feedback and corrections, you help us fix issues and improve the overall quality of the application. We rely on your expertise to identify any gaps or areas needing enhancement.\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent, enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://docs.databricks.com/en/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "tools = []\n",
    "\n",
    "# You can use UDFs in Unity Catalog as agent tools\n",
    "# Below, we add the `system.ai.python_exec` UDF, which provides\n",
    "# a python code interpreter tool to our agent\n",
    "# You can also add local LangChain python tools. See https://python.langchain.com/docs/concepts/tools\n",
    "\n",
    "# Add additional tools\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "uc_tool_names = [f\"{catalog}.{agent_schema}.*\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "# Use Databricks vector search indexes as tools\n",
    "# See https://docs.databricks.com/en/generative-ai/agent-framework/unstructured-retrieval-tools.html\n",
    "# for details\n",
    "\n",
    "# Add vector search indexes\n",
    "vector_search_tools = [\n",
    "        VectorSearchRetrieverTool(\n",
    "          index_name=f\"{catalog}.{demo_schema}.{vectorSearchIndexName}\",\n",
    "          tool_name=\"databricks_docs_retriever\",\n",
    "          tool_description=\"Retrieves information about Databricks products from official Databricks documentation. This must be used\",\n",
    "          columns=[\"id\", \"url\", \"content\"]\n",
    "        )\n",
    "]\n",
    "tools.extend(vector_search_tools)\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "mlflow.langchain.autolog()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0e11172-5eea-411e-9be8-eebe766552ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Test the Langgraph Agent\n",
    "\n",
    "Now that we have a theoretically working Agent. Let's make sure it works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40563d9-0213-4048-b373-fcdd730e9a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474e0a5e-d1e3-4bdb-9c03-2abbff02976d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "result = AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"Hello! what is databricks?\"}]})\n",
    "print(f\"Full Payload which shows the Agent determining what tool to use and the tool it called before providing answer: {result}.\\n\\n**Actual Answer**: {result.messages[-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2d28550-c0ba-4a6d-beca-3a1a96d5b79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Log the Langgraph Agent\n",
    "\n",
    "Now that we have a confirmed working Langgraph Agent, we need to log the model to MLflow. This will capture all the dependencies and necessary credentials we need to deploy the Langgraph Agent.\n",
    "\n",
    "If you plan on using other Databricks features that need credentials, you can use automatic authetication passthrough. This is what our _resources_ variable is for: https://docs.databricks.com/aws/en/generative-ai/agent-framework/deploy-agent#authentication-for-dependent-resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f4f713-3391-4245-b874-f38c8feb539f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import tools, LLM_ENDPOINT_NAME\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool in tools:\n",
    "    if isinstance(tool, VectorSearchRetrieverTool):\n",
    "        resources.extend(tool.resources)\n",
    "    elif isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"langgraph==0.3.4\",\n",
    "            \"databricks-langchain\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "        code_paths=['config_import.py']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9662175-7cb6-4c3b-b2f2-bb2ffad6c7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Register the MLflow Model to Unity Catalog\n",
    "\n",
    "To prepare our model for serving, we must make sure the model is registered to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d116552d-45ab-47c2-8fc9-841efccd76ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config_import import demo_schema, catalog, finalchatBotModelName\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = catalog\n",
    "schema = demo_schema\n",
    "model_name = finalchatBotModelName\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4a4b2bb-ce4b-401d-bc99-465d58f7715b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RAG in Production\n",
    "\n",
    "This workshop is not to show you how to set up RAG on Databricks. Please check out our self paced learning here: <insert link here> \n",
    "\n",
    "You can follow the notebooks in the folder called RAG to set one up. However, this workshop we will demonstrate what it looks like to prepare and monitor your RAG application in Production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b93aae92-00b4-42d7-9923-5db037670783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Evaluate your bot's quality with Mosaic AI Agent Evaluation specialized LLM judge models\n",
    "\n",
    "Evaluation is a key part of deploying a RAG application. Databricks simplify this tasks with specialized LLM models tuned to evaluate your bot's quality/cost/latency, even if ground truth is not available.\n",
    "\n",
    "This Agent Evaluation's specialized AI evaluator is integrated into integrated into `mlflow.evaluate(...)`, all you need to do is pass `model_type=\"databricks-agent\"`.\n",
    "\n",
    "Mosaic AI Agent Evaluation evaluates:\n",
    "1. Answer correctness - requires ground truth\n",
    "2. Hallucination / groundness - no ground truth required\n",
    "3. Answer relevance - no ground truth required\n",
    "4. Retrieval precision - no ground truth required\n",
    "5. (Lack of) Toxicity - no ground truth required\n",
    "\n",
    "In this example, we'll use an evaluation set that we curated based on our internal experts using the Mosaic AI Agent Evaluation review app interface.  This proper Eval Dataset is saved as a Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd1268a-e528-4ab3-92c3-4413bf71a581",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Synthetic Data Evaluation"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import generate_evals_df\n",
    "import mlflow\n",
    "\n",
    "agent_description = \"A chatbot that answers questions about Databricks Documentation.\"\n",
    "question_guidelines = \"\"\"\n",
    "Questions must strictly be about Databricks and its documentation. \n",
    "# User personas\n",
    "- A developer new to the Databricks platform and documentation\n",
    "# Example questions\n",
    "- What API lets me parallelize operations over rows of a delta table?\n",
    "\"\"\"\n",
    "# TODO: Spark/Pandas DataFrame with \"content\" and \"doc_uri\" columns.\n",
    "docs = spark.table(f\"{catalog}.{demo_schema}.databricks_documentation\")\n",
    "docs = docs.withColumnRenamed(\"url\", \"doc_uri\")\n",
    "evals = generate_evals_df(\n",
    "    docs=docs,\n",
    "    num_evals=10,\n",
    "    agent_description=agent_description,\n",
    "    question_guidelines=question_guidelines,\n",
    ")\n",
    "print(evals)\n",
    "eval_result = mlflow.evaluate(data=evals, model=logged_agent_info.model_uri, model_type=\"databricks-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c90cd2-4593-44f1-93c3-41036e286381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset = spark.table(f\"{catalog}.{demo_schema}.eval_set_databricks_documentation\").sample(fraction=0.1, seed=2).limit(10).toPandas()\n",
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd729a36-0cee-4d80-af46-92bb19237f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    # Evaluate the logged model\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=eval_dataset, # Your evaluation set\n",
    "        model=logged_agent_info.model_uri,\n",
    "        model_type=\"databricks-agent\", # active Mosaic AI Agent Evaluation\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "374d78d0-b3ab-477a-a714-13d9fef4e3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Deploy your Agent/Model\n",
    "\n",
    "Databricks provides an easy function called deploy() from the Mosaic AI Agent Framework. This deploys a Model Serving Endpoint on a CPU resource, a Feedback Model and a Review App UI with a shareable link. This is a critical step in review your application as you can send the Review App to subject matter experts for review. They can then provide feedback directly on the app that you can later review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c621502d-39a3-4411-869a-ab0692ddd261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, tags = {\"endpointSource\": \"docs\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f44de57-18bd-43c9-a00e-9288b2efbb40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mosaic AI Model Training for Fine Tuning LLMs \n",
    "\n",
    "We do not expect you for this workshop to fine tune an LLM. However, we will be demonstrating the performance impact of fine-tuning a Llama-1B model through the playground! \n",
    "\n",
    "We trained this model on a dataset containing medical terms. While larger models can handle these words well, the smaller models struggle with them since they are rarely used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc545af-e4d1-45ad-9e0b-3c585b21c3ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 893528767856920,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Langgraph Review",
   "widgets": {
    "agent_schema": {
     "currentValue": "agents",
     "nuid": "e73a2cbd-b149-4d1f-b59d-dfd2111120b4",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "agent_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "agent_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": true,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
