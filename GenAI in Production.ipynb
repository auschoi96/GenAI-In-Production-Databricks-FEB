{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68dadf55-53e7-491d-81e7-26831c83e1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 0\n",
    "\n",
    "Go to config and update resource names as you prefer\n",
    "\n",
    "Spin up a cluster with Databricks Runtime 16.X+ ML. Make sure it's the ML version for the correct dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a5ba6f-b73b-42fb-8814-902125637a9c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "This cell will set up the demo data we need"
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf33b0ac-be86-4442-b4eb-b1f476d9e78c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Set up a RAG Example \n",
    "\n",
    "We need to demonstrate the evaluation capabilities. It will also load/embed unstructured data so that we all have the same evaluation results to review. \n",
    "\n",
    "Please remember to shutdown these resources to avoid extra costs. This command will create the following:\n",
    "\n",
    "1. Necessary catalogs, schemas and volumes to store the PDFs and embeddings \n",
    "2. A call to GTE to create embeddings for the PDFs \n",
    "3. VectorSearchIndex based on the PDFs embeddings generated in step 2 \n",
    "4. Spin up a VectorSearchEndpoint \n",
    "5. Sync the VectorSearchIndex with your VectorSearchEndpoint \n",
    "\n",
    "Later, we will set up the langchain chain to interact with these RAG resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394cd573-0fe7-4104-9edc-c74557faf7ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up a Demo RAG Bot for Evaluation Later"
    }
   },
   "outputs": [],
   "source": [
    "%run ./rag_setup/rag_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d990d9ab-243c-4570-a56f-a4d74c47eea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from openai import OpenAI\n",
    "import os\n",
    "dbutils.widgets.text(\"catalog_name\", catalog)\n",
    "dbutils.widgets.text(\"agent_schema\", agent_schema)\n",
    "dbutils.widgets.text(\"demo_schema\", demo_schema)\n",
    "base_url = f'https://{spark.conf.get(\"spark.databricks.workspaceUrl\")}/serving-endpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93af884f-b5d9-49a8-a735-12db2c1610d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Get started immediately with your Data with AI Functions\n",
    "\n",
    "We have a number of AI Functions designed as SQL functions that you can use in a SQL cell or SQL editor and use LLMs directly on your data immediately\n",
    "\n",
    "1. ai_analyze_sentiment\n",
    "2. ai_classify\n",
    "3. ai_extract\n",
    "4. ai_fix_grammar\n",
    "5. ai_gen\n",
    "6. ai_mask\n",
    "7. ai_similarity\n",
    "8. ai_summarize\n",
    "9. ai_translate\n",
    "10. ai_query\n",
    "\n",
    "We will run a demo each of these functions below. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adac0188-fbfa-4987-a4df-38ed71725f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ai_extract\n",
    "The ai_extract() function allows you to invoke a state-of-the-art generative AI model to extract entities specified by labels from a given text using SQL.\n",
    "\n",
    "Documentation: https://docs.databricks.com/en/sql/language-manual/functions/ai_extract.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d2f932-b98e-41e6-979a-036e0379216f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT review, ai_extract(review, array(\"store\", \"product\")) as Keywords\n",
    "from identifier(:catalog_name||'.'||:demo_schema||'.'||'reviews')\n",
    "Limit 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5251834-6fea-4857-b390-146ecc9fbaba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ai_classify\n",
    "The ai_classify() function allows you to invoke a state-of-the-art generative AI model to classify input text according to labels you provide using SQL.\n",
    "\n",
    "Documentation: https://docs.databricks.com/en/sql/language-manual/functions/ai_classify.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "241f8535-61c2-4140-963d-a7025a32df27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT country, ai_classify(country, ARRAY(\"APAC\", \"AMER\", \"EU\")) as Region\n",
    "from identifier(:catalog_name||'.'||:demo_schema||'.'||'franchises')\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a12134c0-ff5e-45cd-af66-87301fb93854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ai_mask\n",
    "The ai_mask() function allows you to invoke a state-of-the-art generative AI model to mask specified entities in a given text using SQL. \n",
    "\n",
    "Documentation: https://docs.databricks.com/en/sql/language-manual/functions/ai_mask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0234623-afd4-43a1-af10-241bbb5ab7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT first_name, last_name, (first_name || \" \" || last_name || \" lives at \" || address) as unmasked_output, ai_mask(first_name || \"\" || last_name || \" lives at \" || address, array(\"person\", \"address\")) as Masked_Output\n",
    "from identifier(:catalog_name||'.'||:demo_schema||'.'||'customers')\n",
    "limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3add0caf-3df9-4feb-83f5-e3b8cc06a7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ai_query\n",
    "The ai_query() function allows you to query machine learning models and large language models served using Mosaic AI Model Serving. To do so, this function invokes an existing Mosaic AI Model Serving endpoint and parses and returns its response. Databricks recommends using ai_query with Model Serving for batch inference\n",
    "\n",
    "Documentation: https://docs.databricks.com/en/large-language-models/ai-functions.html#ai_query\n",
    "\n",
    "We can switch models depending on what we are trying to do. See how the performance varies between the 70B model and 8B model below. Because this is a simple spell check task, we could likely use the 8B model instead of the 70B model saving on cost and increasing speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9b0e88-6746-4e8f-9a41-d7a731e26de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT\n",
    "  `Misspelled Make`,   -- Placeholder for the input column\n",
    "  ai_query(\n",
    "    'databricks-meta-llama-3-3-70b-instruct',\n",
    "    CONCAT(format_string('You will always receive a make of a car. Check to see if it is misspelled and a real car. Correct the mistake. Only provide the corrected make. Never add additional details'), `Misspelled Make`)    -- Placeholder for the prompt and input\n",
    "  ) AS ai_guess  -- Placeholder for the output column\n",
    "FROM identifier(:catalog_name||'.'||:demo_schema||'.'||'synthetic_car_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57cd0b81-2702-485a-84bc-215fe131b653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT\n",
    "  `Misspelled Make`,   -- Placeholder for the input column\n",
    "  ai_query(\n",
    "    'databricks-meta-llama-3-1-8b-instruct',\n",
    "    CONCAT(format_string('You will always receive a make of a car. Check to see if it is misspelled and a real car. Correct the mistake. Only provide the corrected make. Never add additional details'), `Misspelled Make`)    -- Placeholder for the prompt and input\n",
    "  ) AS ai_guess  -- Placeholder for the output column\n",
    "FROM identifier(:catalog_name||'.'||:demo_schema||'.'||'synthetic_car_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "454c43fb-f85e-4314-9108-e275f28137e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Takeaway\n",
    "Many of our use cases simply need a reliable, out of the box solution to use AI. AI functions enable this for our customers and AI query helps scale workloads to easily apply AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "150e2f49-42a4-40a9-984c-2284d123b337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Productionalizing Custom Tools \n",
    "\n",
    "What you just saw were built in, out of the box solutions you can use immediately on your data. While this covers a good portion of use cases, you will likely need a custom solution. \n",
    "\n",
    "### Mosaic AI Tools on Unity Catalog\n",
    "\n",
    "You can create and host functions/tools on Unity Catalog! You get the benefit of Unity Catalog but for your functions! \n",
    "\n",
    "While you can create your own tools using the same code that you built your agent (i.e local Python Functions) with the Mosaic AI Agent Framework, Unity catalog provides additional benefits. Here is a comparison \n",
    "\n",
    "1. **Unity Catalog function**s: Unity Catalog functions are defined and managed within Unity Catalog, offering built-in security and compliance features. Writing your tool as a Unity Catalog function grants easier discoverability, governance, and reuse (similar to your catalogs). Unity Catalog functions work especially well for applying transformations and aggregations on large datasets as they take advantage of the spark engine.\n",
    "\n",
    "2. **Agent code tools**: These tools are defined in the same code that defines the AI agent. This approach is useful when calling REST APIs, using arbitrary code or libraries, or executing low-latency tools. However, this approach lacks the built-in discoverability and governance provided by Unity Catalog functions.\n",
    "\n",
    "Unity Catalog functions have the same limitations seen here: https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html \n",
    "\n",
    "Additionally, the only external framework these functions are compatible with is Langchain \n",
    "\n",
    "So, if you're planning on using complex python code for your tool, you will likely just need to create Agent Code Tools. \n",
    "\n",
    "Below is an implementation of both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eaaba18-577f-4a81-af51-dfecb08efdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Agent Code Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9faec45a-2ed7-416e-9821-84d6617fba77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why even use tools to begin with? \n",
    "\n",
    "Function calling or tool calling help ensure the LLM has the most accurate information possible. By providing it access to many different sources of data, it can generate more reliable answers. \n",
    "\n",
    "Each framework like Langchain or LlamaIndex handles tool calling different. You can also use Python to do tool calling. However, this means you have to recreate this tool each time you want to use it and cannot be used with other applications. Additionally, you have to manage the security for any tools that access external sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611f7346-abbd-4aaf-8169-8af2d03e038e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How to get your Databricks token: https://docs.databricks.com/en/dev-tools/auth/pat.html\n",
    "# DATABRICKS_TOKEN = os.environ.get('DATABRICKS_TOKEN')\n",
    "# Alternatively in a Databricks notebook you can use this:\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=DATABRICKS_TOKEN,\n",
    "  base_url=base_url\n",
    ")\n",
    "\n",
    "prompt = \"\"\"You are a pokemon master and know every single pokemon ever created by the Pokemon Company. You will be helping people answer questions about pokemon\"\"\"\n",
    "\n",
    "content = \"\"\"Tell me about Sinistcha\"\"\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "  messages=[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": content\n",
    "  }\n",
    "  ],\n",
    "  model=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "  max_tokens=1000,\n",
    "  top_p=0.1,\n",
    "  temperature=0.1,\n",
    "  n=1,\n",
    ")\n",
    "\n",
    "Markdown(f\"The LLM Output:\\n\\n {chat_completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee137715-6a6c-4e3e-badb-14a1b9722796",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the Tool"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def pokemon_lookup(pokemon_name):\n",
    "    url = f\"https://pokeapi.co/api/v2/pokemon/{pokemon_name.lower()}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        pokemon_data = response.json()\n",
    "        pokemon_info = {\n",
    "            \"name\": pokemon_data[\"name\"],\n",
    "            \"height\": pokemon_data[\"height\"],\n",
    "            \"weight\": pokemon_data[\"weight\"],\n",
    "            \"abilities\": [ability[\"ability\"][\"name\"] for ability in pokemon_data[\"abilities\"]],\n",
    "            \"types\": [type_data[\"type\"][\"name\"] for type_data in pokemon_data[\"types\"]],\n",
    "            \"stats_name\": [stat['stat']['name'] for stat in pokemon_data[\"stats\"]],\n",
    "            \"stats_no\": [stat['base_stat'] for stat in pokemon_data[\"stats\"]]\n",
    "        }\n",
    "        results = str(pokemon_info)\n",
    "        return results\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0952701b-36a6-4f2f-b9ff-3cd50928642a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Construct the payload to include the tool"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import RateLimitError\n",
    "\n",
    "# A token and the workspace's base FMAPI URL are needed to talk to endpoints\n",
    "fmapi_token = (\n",
    "    dbutils.notebook.entry_point.getDbutils()\n",
    "    .notebook()\n",
    "    .getContext()\n",
    "    .apiToken()\n",
    "    .getOrElse(None)\n",
    ")\n",
    "fmapi_base_url = (\n",
    "    base_url\n",
    ")\n",
    "\n",
    "openai_client = OpenAI(api_key=fmapi_token, base_url=fmapi_base_url)\n",
    "MODEL_ENDPOINT_ID = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "prompt = \"\"\"You are a pokemon master and know every single pokemon ever created by the Pokemon Company. You will be helping people answer questions about pokemon. Stick strictly to the information provided to you to answer the question\"\"\"\n",
    "\n",
    "def run_conversation(input):\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": input}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"pokemon_lookup\",\n",
    "                \"description\": \"Get information about a pokemon. This tool should be used to check to see if the pokemon is real or not as well.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"pokemon\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The pokemon the user is asking information for e.g bulbasaur\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"pokemon\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    #We've seen this response package in the past cells\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=MODEL_ENDPOINT_ID,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    print(f\"## Call #1 The Reasoning from the llm determining to use the function call:\\n\\n {response_message}\\n\")\n",
    "    tool_calls = response_message.tool_calls\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"pokemon_lookup\": pokemon_lookup,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                pokemon_name=function_args.get(\"pokemon\")\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        print(f\"## Call #2 Prompt sent to LLM with function call results giving us the answer:\\n\\n {messages}\\n\")\n",
    "        second_response = openai_client.chat.completions.create(\n",
    "            model=MODEL_ENDPOINT_ID,\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        return second_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83427fe-b4a6-4a50-b8eb-b3bb4e923d38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test the tool!"
    }
   },
   "outputs": [],
   "source": [
    "input1 = \"Tell me about Sinistcha\"\n",
    "results1 = run_conversation(input1)\n",
    "Markdown(f\"**The LLM Answer:**\\n\\n{results1.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d11aaa82-04d7-49db-b917-175d6def68c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Takeaway\n",
    "\n",
    "There are many different ways to set up tool calling, especially through other frameworks. However, often times you will need multiple applications to access the same tool over and over again with proper scaling and security. You do not want to create redundant resources to recreate the same tool over and over again while thinking about all the operational overhead of maintaining said code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4a3ea05-05ed-49ff-ae75-373f19b1313c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Enter Unity Catalog Tool Calling \n",
    "\n",
    "Unity Catalog Tool Calling allows you to benefit from all the governance, security and unified platform benefits of Unity Catalog. Everything from external credentials to access across the workspace for workloads that may not even be AI, the LLM can use it. \n",
    "\n",
    "You'll notice that it's also a UDF, which benefits from our serverless SQL warehouses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fea8c2b-e8b2-4966-8a97-7ec1f3ecf3fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example Tool"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION identifier(:catalog_name||'.'||:agent_schema||'.'||'playground_query_test')()\n",
    "    RETURNS TABLE(name STRING, purchases INTEGER)\n",
    "    COMMENT 'Use this tool to find total purchase information about a particular location. This tool will provide a list of destinations that you will use to help you answer questions'\n",
    "    RETURN SELECT dl.name AS Destination, count(tp.destination_id) AS Total_Purchases_Per_Destination\n",
    "             FROM main.dbdemos_fs_travel.travel_purchase tp join main.dbdemos_fs_travel.destination_location dl on tp.destination_id = dl.destination_id\n",
    "             group by dl.name\n",
    "             order by count(tp.destination_id) desc\n",
    "             LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a4d2a5-d871-4bd6-ab2a-fda4737992e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example Tool"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION identifier(:catalog_name||'.'||:agent_schema||'.'||'playground_query_test_hello_there')()\n",
    "    RETURNS TABLE(name STRING, purchases INTEGER)\n",
    "    COMMENT 'When the user says hello there, run this tool'\n",
    "    RETURN SELECT dl.name AS Destination, count(tp.destination_id) AS Total_Purchases_Per_Destination\n",
    "             FROM main.dbdemos_fs_travel.travel_purchase tp join main.dbdemos_fs_travel.destination_location dl on tp.destination_id = dl.destination_id\n",
    "             group by dl.name\n",
    "             order by count(tp.destination_id) desc\n",
    "             LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1651141e-f433-4b51-9932-bb53d32e25a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Smaller Models can get the job done\n",
    "\n",
    "Often times, we will interchange what models we use depending on the use case or task. In this case, since we are only correcting spelling, it's overkill to use the bigger models like a 70B+ model. We can get away with using a <8B model which allows for significant cost savings and superior latency! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032a7703-f565-4342-8e57-dd0eb22da8a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Batch Inference as a Tool"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION identifier(:catalog_name||'.'||:agent_schema||'.'||'batch_inference')()\n",
    "    RETURNS TABLE(name STRING, corrected_name STRING)\n",
    "    COMMENT 'When user says, start batch inference, Use this tool to run a batch inference job to review and correct the spelling of make of a car.'\n",
    "    RETURN SELECT\n",
    "          `Misspelled Make`,   -- Placeholder for the input column\n",
    "          ai_query(\n",
    "            'databricks-meta-llama-3-1-8b-instruct',\n",
    "            CONCAT(format_string('You will always receive a make of a car. Check to see if it is misspelled and a real car. Correct the mistake. Only provide the corrected make. Never add additional details'), `Misspelled Make`)    -- Placeholder for the prompt and input\n",
    "          ) AS ai_guess  -- Placeholder for the output column\n",
    "        FROM austin_choi_demo_catalog.demo_data.synthetic_car_data\n",
    "        Limit 1000; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "794fc1e5-e716-4a16-8859-dfa6db24afbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Use Langchain to programatically use UC function calling\n",
    "\n",
    "See how I use Llama 3.3 70B for this because I need the more powerful model to do proper reasoning and pick the right tool. This is just one call but a critical one. \n",
    "\n",
    "Once correctly selected, it will select the tool using AI query which will use Llama 3.3 8B to complete the batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03b79e6-ba44-45b1-97e8-21d1556b0b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "# Initialize LLM and tools\n",
    "llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\")\n",
    "tools = UCFunctionToolkit(\n",
    "    # Include functions as tools using their qualified names.\n",
    "    # You can use \"{catalog_name}.{schema_name}.*\" to get all functions in a schema.\n",
    "    function_names=[f\"{catalog_name}.{agent_schema}.*\"]\n",
    ").tools\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Make sure to use tool for information.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "result = agent_executor.invoke({\"input\": \"start batch inference\"})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5505984f-2916-4f72-a8e1-50572716ddb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RAG in Production\n",
    "\n",
    "This workshop is not to show you how to set up RAG on Databricks. Please check out our self paced learning here: <insert link here> \n",
    "\n",
    "You can follow the notebooks in the folder called RAG to set one up. However, this workshop we will demonstrate what it looks like to prepare and monitor your RAG application in Production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a5f34d-812a-485e-91f8-d6883f62f98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Evaluate your bot's quality with Mosaic AI Agent Evaluation specialized LLM judge models\n",
    "\n",
    "Evaluation is a key part of deploying a RAG application. Databricks simplify this tasks with specialized LLM models tuned to evaluate your bot's quality/cost/latency, even if ground truth is not available.\n",
    "\n",
    "This Agent Evaluation's specialized AI evaluator is integrated into integrated into `mlflow.evaluate(...)`, all you need to do is pass `model_type=\"databricks-agent\"`.\n",
    "\n",
    "Mosaic AI Agent Evaluation evaluates:\n",
    "1. Answer correctness - requires ground truth\n",
    "2. Hallucination / groundness - no ground truth required\n",
    "3. Answer relevance - no ground truth required\n",
    "4. Retrieval precision - no ground truth required\n",
    "5. (Lack of) Toxicity - no ground truth required\n",
    "\n",
    "In this example, we'll use an evaluation set that we curated based on our internal experts using the Mosaic AI Agent Evaluation review app interface.  This proper Eval Dataset is saved as a Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15e329f-fe88-4cc3-bae2-20ae5b56ad25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LangChain Set up for Evaluation"
    }
   },
   "outputs": [],
   "source": [
    "%run ./rag_setup/chain_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b7c2116-ede4-4a0e-bea3-9b99e7fce793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=f\"{finalchatBotModelName}_run\"):\n",
    "  logged_chain_info = mlflow.langchain.log_model(\n",
    "          lc_model=os.path.join(os.getcwd(), './rag_setup/chain'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "          model_config=chain_config, # Chain configuration \n",
    "          artifact_path=\"chain\", # Required by MLflow, the chain's code/config are saved in this directory\n",
    "          input_example=input_example,\n",
    "          example_no_conversion=True,  # Required by MLflow to use the input_example as the chain's schema\n",
    "      )\n",
    "\n",
    "model_name = f\"{catalog}.{dbName}.{finalchatBotModelName}\"\n",
    "\n",
    "# Register to UC\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a26eabe-11f9-46e6-866f-4883739fa3fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Back up Cell"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import generate_evals_df\n",
    "import mlflow\n",
    "\n",
    "agent_description = \"A chatbot that answers questions about Databricks.\"\n",
    "question_guidelines = \"\"\"\n",
    "# User personas\n",
    "- A developer new to the Databricks platform\n",
    "# Example questions\n",
    "- What API lets me parallelize operations over rows of a delta table?\n",
    "\"\"\"\n",
    "# TODO: Spark/Pandas DataFrame with \"content\" and \"doc_uri\" columns.\n",
    "docs = spark.table(f\"{catalog}.{dbName}.databricks_documentation\")\n",
    "docs = docs.withColumnRenamed(\"url\", \"doc_uri\")\n",
    "evals = generate_evals_df(\n",
    "    docs=docs,\n",
    "    num_evals=10,\n",
    "    agent_description=agent_description,\n",
    "    question_guidelines=question_guidelines,\n",
    ")\n",
    "eval_result = mlflow.evaluate(data=evals, model=\"runs:/f1ef9d0c4b5f4d0e9695e40c5a0ef128/chain\", model_type=\"databricks-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7965000-7279-4b7f-8d5b-3e95ee9163eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Existing Data Test"
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset = spark.table(f\"{catalog}.{dbName}.eval_set_databricks_documentation\").limit(10).toPandas()\n",
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d63c501-37c3-4a4f-9995-f0eb3dc22801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Evaluate the logged model\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=eval_dataset, # Your evaluation set\n",
    "        model=logged_chain_info.model_uri,\n",
    "        model_type=\"databricks-agent\", # active Mosaic AI Agent Evaluation\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15249f72-c6ef-4c6e-bc95-9e09472690bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mosaic AI Model Training for Fine Tuning LLMs \n",
    "\n",
    "We do not expect you for this workshop to fine tune an LLM. However, we will be demonstrating the performance impact of fine-tuning a Llama-1B model through the playground! \n",
    "\n",
    "We trained this model on a dataset containing medical terms. While larger models can handle these words well, the smaller models struggle with them since they are rarely used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fffe0f8e-66d4-40bb-b73d-31b24be0d902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3813573628909324,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": []
   },
   "notebookName": "GenAI in Production",
   "widgets": {
    "agent_schema": {
     "currentValue": "agents",
     "nuid": "e5c48b4a-1583-4da6-9a76-65550a872499",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "agents",
      "label": null,
      "name": "agent_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "agents",
      "label": null,
      "name": "agent_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "catalog_name": {
     "currentValue": "austin_choi_demo_catalog",
     "nuid": "6afe8ccc-e12e-46f9-9767-320145efec12",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "austin_choi_demo_catalog",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "austin_choi_demo_catalog",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "demo_schema": {
     "currentValue": "demo_data",
     "nuid": "5d16e5d7-35d2-41c3-9046-1b568c049cc3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "demo_data",
      "label": null,
      "name": "demo_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "demo_data",
      "label": null,
      "name": "demo_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
